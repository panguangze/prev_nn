#!/usr/bin/env bash
set -euo pipefail

PIDS=()
cleanup() {
  local sig=${1:-TERM}
  echo "[CLEANUP] Signal $sig. Killing ${#PIDS[@]} worker(s)..." >&2
  for p in "${PIDS[@]:-}"; do
    if kill -0 "$p" 2>/dev/null; then
      # 给进程组发信号（如果是 setsid 启动，PGID=PID）
      kill -"$sig" -"$p" 2>/dev/null || true
    fi
  done
  sleep 1
  for p in "${PIDS[@]:-}"; do
    if kill -0 "$p" 2>/dev/null; then
      kill -9 -"$p" 2>/dev/null || true
    fi
  done
}
trap 'cleanup TERM' TERM
trap 'cleanup INT' INT
trap 'cleanup EXIT' EXIT

if [[ -n "${XLA_FLAGS:-}" ]]; then
  export XLA_FLAGS="$(echo "$XLA_FLAGS" | sed 's/--xla_gpu_enable_triton=false//g' | tr -s ' ')"
fi
export XLA_PYTHON_CLIENT_ALLOCATOR=${XLA_PYTHON_CLIENT_ALLOCATOR:-platform}

PARAMS=${1:?Usage: run_af2_workers.sh params.yaml}
OUTROOT=$(python scripts/get_param_yaml.py "$PARAMS" paths.work_dir)
OUTDIR="$OUTROOT/af2_models"
MPNN_DIR="$OUTROOT/mpnn_seqs"
TEMPLATE_DIR=$(python scripts/get_param_yaml.py "$PARAMS" paths.templates_dir)
mkdir -p "$OUTDIR/fasta" "$OUTDIR/predictions" "$OUTDIR/logs" "$OUTDIR/run"

MASTER_LOG="$OUTDIR/log.txt"; : > "$MASTER_LOG"

USE_TEMPLATE=$(python scripts/get_param_yaml.py "$PARAMS" project.use_template)
NUM_MODELS=$(python scripts/get_param_yaml.py "$PARAMS" af2.with_template.initial.num_models)
NUM_RECYCLES=$(python scripts/get_param_yaml.py "$PARAMS" af2.with_template.initial.num_recycles)
AMBER=$(python scripts/get_param_yaml.py "$PARAMS" af2.with_template.initial.amber_relax)
HALT_ON_FAIL=$(python scripts/get_param_yaml.py "$PARAMS" compute.halt_on_fail 2>/dev/null || echo "false")
HALT_ON_FAIL=$(echo "${HALT_ON_FAIL,,}")

if command -v colabfold_batch >/dev/null 2>&1; then
  COLABFOLD="colabfold_batch"
else
  COLABFOLD="python -m colabfold.batch"
fi
HELP=$($COLABFOLD --help 2>&1 || true)
MODEL_FLAG=""; MODEL_TYPE=""
if echo "$HELP" | grep -q -- "--model-type"; then
  MODEL_FLAG="--model-type"; MODEL_TYPE="alphafold2_multimer_v3"
elif echo "$HELP" | grep -q -- "--models"; then
  MODEL_FLAG="--models"; MODEL_TYPE="AlphaFold2-multimer-v3"
fi

discover_gpus() {
  local cfg; local arr=()
  cfg=$(python scripts/get_param_yaml.py "$PARAMS" compute.gpus --json 2>/dev/null || echo "")
  if [[ -n "$cfg" && "$cfg" != "null" ]]; then
    cfg=$(echo "$cfg" | tr -d '[]"' | tr ',' ' ')
    read -r -a arr <<< "$cfg"
  elif [ -n "${CUDA_VISIBLE_DEVICES:-}" ]; then
    IFS=',' read -r -a arr <<< "$CUDA_VISIBLE_DEVICES"
  elif command -v nvidia-smi >/dev/null 2>&1; then
    mapfile -t arr < <(nvidia-smi --query-gpu=index --format=csv,noheader 2>/dev/null)
  fi
  printf '%s\n' "${arr[@]}"
}
mapfile -t GPUS < <(discover_gpus)
MINFREE=$(python scripts/get_param_yaml.py "$PARAMS" compute.min_free_mem_mb_for_gpu)
VALID_GPUS=()
for g in "${GPUS[@]:-}"; do
  free=$(nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits --id="$g" 2>/dev/null || echo "0")
  if [[ "$free" =~ ^[0-9]+$ ]] && [ "$free" -ge "$MINFREE" ]; then
    echo "[INFO] GPU $g free mem: $free MiB" >> "$MASTER_LOG"
    VALID_GPUS+=("$g")
  else
    echo "[WARN] GPU $g low or unknown free mem ($free MiB), skipped" >> "$MASTER_LOG"
  fi
done
NUM_GPUS=${#VALID_GPUS[@]}
echo "[INFO] Usable GPUs ($NUM_GPUS): ${VALID_GPUS[*]}" | tee -a "$MASTER_LOG"
if [ "$NUM_GPUS" -eq 0 ]; then
  echo "[ERROR] No sufficient GPU memory available." | tee -a "$MASTER_LOG"; exit 1
fi

# 生成 METTL1 序列（与之前相同）
if [ ! -f "./outputs/targets/mettl1_seq.fa" ]; then
  echo "[INFO] METTL1 sequence file not found. Generating..." >> "$MASTER_LOG"
  python - <<'PY'
import json, sys
from Bio.PDB import PDBParser, Polypeptide
from Bio.SeqUtils import seq1

cand_file = "./outputs/targets/interface_candidates.json"
with open(cand_file) as f:
    cand = json.load(f)

pdb = cand["mettl1_target_pdb"]
chain_id = cand["mettl1_chain_id"]

parser = PDBParser(QUIET=True)
structure = parser.get_structure("T", pdb)

custom_map = {"MSE":"M","SEC":"U","PYL":"O"}
seq_chars=[]
for r in structure[0][chain_id]:
    if not Polypeptide.is_aa(r, standard=False):
        continue
    resname = r.get_resname().strip()
    try:
        seq_chars.append(seq1(resname, custom_map=custom_map))
    except KeyError:
        seq_chars.append("X")
seq = "".join(seq_chars)
with open("./outputs/targets/mettl1_seq.fa","w") as f:
    f.write(">METTL1\n"+seq+"\n")
print("[OK] METTL1 seq written.")
PY
else
  echo "[INFO] Found existing METTL1 sequence file." >> "$MASTER_LOG"
fi
METTL1_SEQ=$(grep -v "^>" ./outputs/targets/mettl1_seq.fa | tr -d '[:space:]' || true)
if [[ -z "${METTL1_SEQ:-}" ]]; then
  echo "[ERROR] Empty METTL1 sequence." | tee -a "$MASTER_LOG"; exit 1
fi

# 组装 FASTA
echo "[INFO] Assembling FASTA..." | tee -a "$MASTER_LOG"
rm -rf "$OUTDIR/fasta"
mkdir -p "$OUTDIR/fasta"
find "$MPNN_DIR" -type f -path "*/seqs/*.fa" | sort | while read -r mpnn_multiseq_fa; do
  [[ ! -s "$mpnn_multiseq_fa" ]] && continue
  design_backbone_name=$(basename "${mpnn_multiseq_fa%.fa}")
  awk -v mettl1_seq="$METTL1_SEQ" -v out_dir="$OUTDIR/fasta" -v backbone_name="$design_backbone_name" \
      'BEGIN{RS=">";FS="\n"}NR>1{header=$1;sequence="";for(i=2;i<=NF;i++){sequence=sequence $i}gsub(/[ \t\r\n]/,"",sequence);if(sequence=="")next;if(match(header,/sample=([^, ]+)/,arr)){sample_id=arr[1]}else{sample_id="0"}out_file=out_dir"/"backbone_name"_sample_"sample_id".fa";print ">METTL1:"backbone_name"_sample_"sample_id > out_file;print mettl1_seq":"sequence >> out_file;close(out_file)}' "$mpnn_multiseq_fa"
done

mapfile -t ALL_FASTAS < <(find "$OUTDIR/fasta" -type f -name "*.fa" | sort)
NUM_FILES=${#ALL_FASTAS[@]}
if [[ "$NUM_FILES" -eq 0 ]]; then
  echo "[ERROR] No FASTA files assembled." | tee -a "$MASTER_LOG"; exit 1
fi
echo "[INFO] Total $NUM_FILES FASTA" | tee -a "$MASTER_LOG"

# 任务队列
QUEUE="$OUTDIR/run/task_queue.tsv"; : > "$QUEUE"
for fasta in "${ALL_FASTAS[@]}"; do
  base=$(basename "${fasta%.fa}")
  outdir="$OUTDIR/predictions/${base}"
  logfn="$OUTDIR/logs/${base}.log"
  mkdir -p "$outdir"
  printf "%s\t%s\t%s\n" "$fasta" "$outdir" "$logfn" >> "$QUEUE"
done
TOTAL_TASKS=$(wc -l < "$QUEUE" | tr -d '[:space:]')
echo "[INFO] Task queue at $QUEUE with $TOTAL_TASKS tasks" | tee -a "$MASTER_LOG"

LOCK="$OUTDIR/run/queue.lock"; : > "$LOCK"

# 参数组装
TEMPLATE_ARGS=()
if [[ "${USE_TEMPLATE,,}" == "true" ]]; then
  TEMPLATE_ARGS=( --templates --custom-template-path "$TEMPLATE_DIR" )
fi
RELAX_ARGS=()
if [[ "${AMBER,,}" == "true" ]]; then
  if echo "$HELP" | grep -q -- "--amber"; then RELAX_ARGS+=( --amber ); fi
  if echo "$HELP" | grep -q -- "--use-gpu-relax"; then RELAX_ARGS+=( --use-gpu-relax ); fi
  RELAX_ARGS+=( --num-relax 1 )
fi
INFER_ARGS=(
  ${MODEL_FLAG:+$MODEL_FLAG} ${MODEL_FLAG:+$MODEL_TYPE}
  --num-recycle "$NUM_RECYCLES"
  --num-models "$NUM_MODELS"
  --msa-mode single_sequence
  --overwrite-existing-results
)

# 写 worker 脚本：数组执行，避免 bash -lc
WORKER="$OUTDIR/run/worker.sh"
cat > "$WORKER" <<'EOF'
#!/usr/bin/env bash
set -eo pipefail  # 不启用 -u，避免 unbound variable
GPU_ID=${1:?gpu_id required}
QUEUE=${2:?queue file}
LOCK=${3:?lock file}
COLABFOLD=${4:?colabfold cmd}
MASTER_LOG=${5:?master log}
shift 5
# 剩余参数：INFER_ARGS + TEMPLATE_ARGS + RELAX_ARGS（均为单词化展开）

export CUDA_VISIBLE_DEVICES="$GPU_ID"
WORK_LOG_DIR=$(dirname "$MASTER_LOG")/workers
mkdir -p "$WORK_LOG_DIR"
WLOG="$WORK_LOG_DIR/worker_gpu_${GPU_ID}.log"; : > "$WLOG"
echo "[WORKER:$GPU_ID] Start at $(date +"%F %T"))" | tee -a "$WLOG" >> "$MASTER_LOG"

pop_task() {
  exec 9<>"$LOCK"
  flock -x 9
  local line
  line="$(head -n 1 "$QUEUE" || true)"
  if [[ -z "$line" ]]; then
    flock -u 9
    exec 9>&-
    echo ""
    return 0
  fi
  tail -n +2 "$QUEUE" > "$QUEUE.tmp" && mv "$QUEUE.tmp" "$QUEUE"
  flock -u 9
  exec 9>&-
  echo "$line"
}

while true; do
  task="$(pop_task)"
  if [[ -z "$task" ]]; then
    echo "[WORKER:$GPU_ID] No more tasks. Exit." | tee -a "$WLOG" >> "$MASTER_LOG"
    break
  fi
  IFS=$'\t' read -r FASTA OUTDIR LOGF <<< "$task"
  mkdir -p "$(dirname "$LOGF")" "$OUTDIR"

  {
    echo "============================================================"
    echo "[WORKER:$GPU_ID] $(date +"%F %T")"
    echo "FASTA=$FASTA"
    echo "OUTDIR=$OUTDIR"
    echo "LOG=$LOGF"
    echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"
  } | tee -a "$WLOG" >> "$MASTER_LOG"

  # 构造命令数组：COLABFOLD + args + FASTA + OUTDIR
  cmd=( "$COLABFOLD" "$@" "$FASTA" "$OUTDIR" )
  {
    printf "[WORKER:%s] CMD:" "$GPU_ID"
    printf " %q" "${cmd[@]}"
    printf "\n"
  } | tee -a "$WLOG" >> "$MASTER_LOG"

  # 前台执行，日志独立写入
  if ! "${cmd[@]}" >> "$LOGF" 2>&1; then
    echo "[WORKER:$GPU_ID] Task FAILED: $FASTA" | tee -a "$WLOG" >> "$MASTER_LOG"
    echo "$FASTA" >> "$(dirname "$MASTER_LOG")/run/failed.list"
  else
    echo "[WORKER:$GPU_ID] Task OK: $FASTA" | tee -a "$WLOG" >> "$MASTER_LOG"
  fi
done
EOF
chmod +x "$WORKER"

# 准备参数扁平化（数组 -> 单词序列）
FLAT_ARGS=( "${INFER_ARGS[@]}" "${TEMPLATE_ARGS[@]}" "${RELAX_ARGS[@]}" )

echo "[INFO] Launching $NUM_GPUS worker(s)..." | tee -a "$MASTER_LOG"
for i in "${!VALID_GPUS[@]}"; do
  GPU_ID=${VALID_GPUS[$i]}
  # 用 setsid -w 启动，使其成为独立进程组，且仍是本 shell 的直接子进程
  setsid -w "$WORKER" "$GPU_ID" "$QUEUE" "$LOCK" "$COLABFOLD" "$MASTER_LOG" "${FLAT_ARGS[@]}" >/dev/null 2>&1 &
  PID=$!
  PIDS+=("$PID")
  echo "[INFO] Worker for GPU $GPU_ID started, PID=$PID" | tee -a "$MASTER_LOG"
done

FAIL_FILE="$OUTDIR/run/failed.list"; : > "$FAIL_FILE"
FAILS_PREV=0
monitor_loop() {
  while true; do
    alive=0
    for p in "${PIDS[@]:-}"; do
      if kill -0 "$p" 2>/dev/null; then
        alive=$((alive+1))
      fi
    done
    if [[ $alive -eq 0 ]]; then
      break
    fi
    if [[ "${HALT_ON_FAIL}" == "true" ]]; then
      fails_now=$(wc -l < "$FAIL_FILE" | tr -d '[:space:]')
      if [[ "$fails_now" -gt "$FAILS_PREV" ]]; then
        echo "[MASTER] Detected failure and HALT_ON_FAIL=true. Stopping all workers..." | tee -a "$MASTER_LOG"
        cleanup TERM
        break
      fi
      FAILS_PREV=$fails_now
    fi
    sleep 2
  done
}
monitor_loop

RET=0
for p in "${PIDS[@]:-}"; do
  if ! wait "$p"; then RET=1; fi
done
trap - EXIT

FAILS=$(wc -l < "$FAIL_FILE" | tr -d '[:space:]')
echo "[DONE] Workers finished. Total tasks: $TOTAL_TASKS, Failures: $FAILS" | tee -a "$MASTER_LOG"
exit $RET
